# Text Extraction
from PyPDF2 import PdfReader
reader = PdfReader (chemistry.pdf)
print(len(reader.pages))
page = reader.pages[40, 41, 42, 43, 44]
text = page.extract_text()
print (text)

#Text Summarisation
import nltk
from string import punctuation
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

value = read_article(file_name)
value

def read_article(file_name):
    file = open(file_name, "r",encoding='utf-8')
    filedata = file.read()
    return filedata

#Tokenization and Cleaning of Data
tokens = word_tokenize(value)
nltk.download("stopwords")

stop_words = stopwords.words('english')

#Word Tokenization and Frequency
word_frequencies = {}
for word in tokens:
    if word.lower() not in stop_words:
        if word.lower() not in punctuation:
            if word not in word_frequencies.keys():
                word_frequencies[word] = 1
            else:
                word_frequencies[word] += 1

max_frequency = max(word_frequencies.values())
print(max_frequency)

for word in word_frequencies.keys():
    word_frequencies[word] = word_frequencies[word]/max_frequency
print(word_frequencies)

#Sentence Tokenization and Frequency
sent_token = sent_tokenize(value)
sent_token

sentence_scores = {}
for sent in sent_token:
    sentence = sent.split(" ")
    for word in sentence:
        if word.lower() in word_frequencies.keys():
            if sent not in sentence_scores.keys():
                sentence_scores[sent] = word_frequencies[word.lower()]
            else:
                sentence_scores[sent] += word_frequencies[word.lower()]
sentence_scores

#Creation of Summary
from heapq import nlargest
select_length = int(len(sent_token)*0.3)
select_length

summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)

len(final_summary)
final_summary = [word for word in summary]
summary = ' '.join(final_summary)
final_summary = [word for word in summary]
summary = ' '.join(final_summary)
